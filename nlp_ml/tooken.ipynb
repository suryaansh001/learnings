{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= \"\"\" Welcome to jklu!.We hope u like  the campus.\n",
    "All the best !.Please pay attention to the following points.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_tokenize works:\n",
    "Input: It takes a string (typically a paragraph of text).\n",
    "\n",
    "Processing: It uses rules to detect sentence boundaries based on punctuation marks like periods, exclamation points, and question marks.\n",
    "\n",
    "Output: It returns a list of sentences as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      "  Welcome to jklu!.We hope u like  the campus.\n",
      "All the best !.Please pay attention to the following points.\n",
      "\n",
      "Tokenized Sentences:\n",
      " [' Welcome to jklu!.We hope u like  the campus.', 'All the best !.Please pay attention to the following points.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Corpus:\\n\", corpus)\n",
    "# Tokenizing the sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(\"\\nTokenized Sentences:\\n\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'How are you doing today?', \"I hope you're having a good day.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sury/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello! How are you doing today? I hope you're having a good day.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      " Welcome to jklu!.We hope u like  the campus.\n",
      "All the best !.Please pay attention to the following points.\n"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "print(type(documents))\n",
    "# Output: list\n",
    "for sent in documents:\n",
    "    print(sent)\n",
    "# Output: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of Tokenization Process:\n",
    "Spaces: Spaces between words are key to distinguishing word boundaries, so any space between characters typically indicates a boundary between words.\n",
    "\n",
    "Punctuation Marks: Common punctuation marks (like ., ,, !, ?, etc.) are treated as separate tokens. This allows the tokenizer to handle sentences properly and ensures that punctuation is not merged with words.\n",
    "\n",
    "Special Characters and Numbers: Other elements, such as numbers or special characters, are treated as separate tokens too. For example, 100 or $5 would both be tokenized as individual tokens.\n",
    "\n",
    "Contractions and Apostrophes: Contractions like \"you're\" or \"doesn't\" are split into separate tokens based on common linguistic rules. For example, \"you're\" would be split into ['you', \"'re\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '!', 'you', 'have', 'to', 'pay', '$', '1000.00', '!', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1=\"hello ! you have to pay $1000.00!!\"\n",
    "word_tokenize(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " '!',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'resident',\n",
       " 'of',\n",
       " 'U.S.A.',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'student',\n",
       " 'of',\n",
       " 'J.K.L.U',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " '2nd',\n",
       " 'year',\n",
       " ',',\n",
       " 'studying',\n",
       " 'B.Tech',\n",
       " '.',\n",
       " 'in',\n",
       " 'CSE',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2=\"hello ! i am a resident of U.S.A. and i am a student of J.K.L.U. now i am in 2nd year,studying B.Tech. in CSE.\"\n",
    "word_tokenize(example2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Issues:\n",
    "Tokenization of Punctuation: In certain cases, punctuation marks might not be tokenized correctly (e.g., in cases involving abbreviations like \"Dr.\" or \"U.S.\"). Some users handle these manually with regular expressions or customized tokenizers.\n",
    "\n",
    "Contractions Handling: If the text uses a lot of contractions (e.g., \"didn't\" or \"I'm\"), it can sometimes be split in ways that may not be ideal, depending on your application. However, in general, it's helpful for processing text in languages like English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'John',\n",
       " 'Smith',\n",
       " 'is',\n",
       " 'a',\n",
       " 'professor',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'California',\n",
       " ',',\n",
       " 'Berkeley',\n",
       " '.',\n",
       " 'He',\n",
       " 'specializes',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example3=\"Dr. John Smith is a professor at the University of California, Berkeley. He specializes in machine learning and artificial intelligence.\"\n",
    "word_tokenize(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "example4=\"hello ! you have to pay $1000.00!!\"\n",
    "wordpunct_tokenize(example3)\n",
    "#here pucntuation  like he's gets to he , ' ,s\"\n",
    "#full stop gets to full stop\n",
    "example5=\"hello ! i am a resident of U.S.A. and i am a student of J.K.L.U. now i am in 2nd year,studying B.Tech. in CSE.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
